{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MRL-AI-774M",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_"
      },
      "source": [
        "#  Monero Research Lab text generator\n",
        "\n",
        "Isthmus / Mitchell, December 2020\n",
        "\n",
        "Base on the [GPT-2 tutorial](https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce) notebook by [Max Woolf](http://minimaxir.com)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhQTe2VT5qP-"
      },
      "source": [
        "## Settings and parameters\n",
        "\n",
        "Note:\n",
        "* `124M` (default): the \"small\" model, 500MB on disk.\n",
        "* `355M`: the \"medium\" model, 1.5GB on disk.\n",
        "* `774M`: the \"large\" model,\n",
        "* `1558M`: the \"extra large\", true model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quBo2MATa-ky"
      },
      "source": [
        "model_size = \"774M\"\n",
        "this_run_name = 'run1' + model_size\n",
        "logs_URL = \"https://raw.githubusercontent.com/Mitchellpkt/log_based_text_generator/main/mrl_logs_cleaned.txt\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYogBESn5jSQ"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBkpRgBCBS2_",
        "outputId": "4b5a4d4c-c32d-48ca-f3f9-0e2c73ae81a2"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "import re "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwVBJi3hcupv"
      },
      "source": [
        "Peep the instance specs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUmTooTW3osf",
        "outputId": "2f62ce02-fef2-425b-bc20-044613e854f0"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Dec 26 23:31:31 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    23W / 300W |      0MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS"
      },
      "source": [
        "## Load GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8wSlgXoDPCR",
        "outputId": "2851af99-7ea0-4c69-97f4-738bbcb52e70"
      },
      "source": [
        "gpt2.download_gpt2(model_name=model_size)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 552Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 123Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 646Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 3.10Git [00:29, 104Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 535Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 2.10Mit [00:00, 227Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 233Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHbSG731Jygg"
      },
      "source": [
        "## Data wrangling MRL logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-207mhmbtZj"
      },
      "source": [
        "Retrieve the IRC logs from the URL of a plaintext data dump"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xhg0oPJhJzLe"
      },
      "source": [
        "import requests\n",
        "\n",
        "file_name = \"mrl.txt\"\n",
        "\n",
        "url = logs_URL\n",
        "data = requests.get(url)\n",
        "manip_text_raw = str(data.text)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jVzUQ5NdjO0",
        "outputId": "33eee85b-1ada-4512-d878-985cfda37b46"
      },
      "source": [
        "# Head of data look alright?\n",
        "print(manip_text_raw[0:3000])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " <ukoehb> is transaction fee 8 bytes?\n",
            " <moneromooo> it is a 64 bit value. it is typically encoded as a varint, if that's what you're asking.\n",
            " <ukoehb> just looking at storage required\n",
            " <ukoehb> varint = variable length integer, so is storage not constant?\n",
            " <moneromooo> yes.\n",
            " <ukoehb> thanks :)\n",
            " <serhack> morning :)\n",
            " <suraenoether> monero coffee chat yall~\n",
            " <sarang> how did the coffee chat go?\n",
            " <sarang> i had a volunteer commitment during that time\n",
            " <sarang> we repair bikes and donate them to veterans and kids who need them\n",
            " <sneurlax1> good with bikes, eh?\n",
            " <sarang> i worked part-time as a mechanic for a few years\n",
            " <sneurlax1> i missed the meeting so have no useful comment there sorry.\n",
            " <sarang> fixing bikes is a ton of fun\n",
            " <sneurlax1> i skipped straight to motorcycles and need to get handy with it quickly\n",
            " — sarang is moving bike convo to #monero-research-lounge \n",
            " <needmoney90> my call with bisq is wednesday, would anyone be available to chat about the technical details of how multisig can be feasibly integrated into bisq transfers?\n",
            " <needmoney90> i'm not sure what or how much is needed\n",
            " <moneromooo> they need n-1/n ?\n",
            " <needmoney90> yup\n",
            " <needmoney90> except i want to set merkato and them up as automated parties\n",
            " <needmoney90> other than the case of a dispute\n",
            " <needmoney90> idk how we're going to manage that, but i think it's possible\n",
            " <moneromooo> so what is needed exactly ?\n",
            " <moneromooo> or what are the questions, if known already ?\n",
            " <needmoney90> merkato will run in the background, checking for orders being hit. if orders are hit, the multisig txes are made, and bisq's admins act as the third party.\n",
            " <needmoney90> all is automated unless the buyer is malicious\n",
            " <needmoney90> idk what questions they'll ask\n",
            " <moneromooo> if they're on irc while i'm awake, i can answer if i know.\n",
            " <moneromooo> the annoying thing is that if alice wants to trade with bob, using carol as third party, they all have to make a wallet.\n",
            " <moneromooo> so it can be come heavy quick.\n",
            " <ukoehb> it seems precarious to implement multisig before suraenoether's update\n",
            " <moneromooo> what is to change ?\n",
            " <ukoehb> biggest change afaik is key merge\n",
            " <ukoehb> adding communication rounds for e.g. commit-and-reveal doesn't change protocol\n",
            " <ukoehb> backward compatibility might be easy though, cause it's either type 1 or type 2 merged address\n",
            " <moneromooo> but once an account is created, you can use it with future version, right ?\n",
            " <ukoehb> the difference is a scalar multiple in several places \n",
            " <ukoehb> hm\n",
            " <ukoehb> ill check\n",
            " <ukoehb> ok yes it should work with future version\n",
            " <ukoehb> you can actually separate key merge and threshold signing completely\n",
            " <ukoehb> just calculate hk -> k and store that instead of recomputing every time\n",
            " <ukoehb> then pass your private keys into sign as normal\n",
            " <ukoehb> neat\n",
            " <suraenoether> ukoehb: yep, lots of little speed-up things\n",
            " <suraenoether> or... conveniences, i should say\n",
            " <suraenoether> i'm puzzling about view keys right now, but i think i \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wWMQ-yhbuDJ"
      },
      "source": [
        "Process the text to strip out irrelevant messages (people joining, leaving, etc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj4FVyPqbme9",
        "outputId": "dfe63216-3b0f-4c6e-9d4f-409d51e99cf8"
      },
      "source": [
        "# Drop case\n",
        "# manip_text_raw = manip_text_raw.lower() # drop case\n",
        "\n",
        "# REmove channel notifications\n",
        "words_to_remove = ('mode','timestamp','joined','left','quit','seconds','channel', '#monero-research-lab', '→', 'chanserv')\n",
        "manip_text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", manip_text_raw) # remove timestamps\n",
        "for w in range(len(words_to_remove)):\n",
        "    this_word = words_to_remove[w]\n",
        "    print('Removing lines containing: ' + this_word)\n",
        "    manip_text = re.sub(\".*\"+this_word+\".*\", \"\", manip_text)\n",
        "    \n",
        "# This next block of code is a functional gargabe hack - streamline later\n",
        "max_rows_blank = 200\n",
        "for i in range(max_rows_blank):\n",
        "    search_str = \"\\n\"*(max_rows_blank-i)\n",
        "    manip_text = re.sub(search_str,'\\n',manip_text)\n",
        "    \n",
        "final_string = manip_text\n",
        "\n",
        "# Peep the results\n",
        "print(manip_text[0:1000])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Removing lines containing: mode\n",
            "Removing lines containing: timestamp\n",
            "Removing lines containing: joined\n",
            "Removing lines containing: left\n",
            "Removing lines containing: quit\n",
            "Removing lines containing: seconds\n",
            "Removing lines containing: channel\n",
            "Removing lines containing: #monero-research-lab\n",
            "Removing lines containing: →\n",
            "Removing lines containing: chanserv\n",
            "\n",
            " <ukoehb> is transaction fee 8 bytes?\n",
            " <moneromooo> it is a 64 bit value. it is typically encoded as a varint, if that's what you're asking.\n",
            " <ukoehb> just looking at storage required\n",
            " <ukoehb> varint = variable length integer, so is storage not constant?\n",
            " <moneromooo> yes.\n",
            " <ukoehb> thanks :)\n",
            " <serhack> morning :)\n",
            " <suraenoether> monero coffee chat yall~\n",
            " <sarang> how did the coffee chat go?\n",
            " <sarang> i had a volunteer commitment during that time\n",
            " <sarang> we repair bikes and donate them to veterans and kids who need them\n",
            " <sneurlax1> good with bikes, eh?\n",
            " <sarang> i worked part-time as a mechanic for a few years\n",
            " <sneurlax1> i missed the meeting so have no useful comment there sorry.\n",
            " <sarang> fixing bikes is a ton of fun\n",
            " <sneurlax1> i skipped straight to motorcycles and need to get handy with it quickly\n",
            " — sarang is moving bike convo to #monero-research-lounge \n",
            " <needmoney90> my call with bisq is wednesday, would anyone be available to chat about the technical details of how multi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UDoBoxgbuoi"
      },
      "source": [
        "Write the file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyhpVwZmbm-x"
      },
      "source": [
        "with open(file_name, 'w') as f:\n",
        "  f.write(final_string)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSA1JguVcbDV"
      },
      "source": [
        "Commented out below, code to link gdrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6okFD8VKtS"
      },
      "source": [
        "# gpt2.mount_gdrive()\n",
        "# gpt2.copy_file_from_gdrive(file_name)\n",
        "# gpt2.load_f"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3"
      },
      "source": [
        "## Finetune GPT-2\n",
        "\n",
        "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
        "\n",
        "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
        "\n",
        "\n",
        "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "* **`sample_every`**: Number of steps to print example output\n",
        "* **`print_every`**: Number of steps to print training progress.\n",
        "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
        "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
        "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeXshJM-Cuaf",
        "outputId": "c9dd2619-35e9-43c4-b65b-654a1910065c"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name=model_size,\n",
        "              steps=-1,\n",
        "              restore_from='latest',\n",
        "              run_name= this_run_name,\n",
        "              print_every=10,\n",
        "              sample_every=250,\n",
        "              save_every=500 \n",
        "              )"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
            "Instructions for updating:\n",
            "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
            "Loading checkpoint checkpoint/run1774M/model-10750\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/run1774M/model-10750\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.46s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 1891095 tokens\n",
            "Training...\n",
            "======== SAMPLE 1 ========\n",
            " w and d are chosen by h to be constants for the group order q, and the other members of q are chosen by the same distribution, so txn matching can be done with no additional input except for the pubkey index. in the above situation, r = {0,1} and d = {0,1} \n",
            " <moneromooo> the argument for not including it, is that \"you only have to look once\", and that can't be borne, no ?\n",
            " <moneromooo> it'd also be impractical, since you could make a transaction for that output.\n",
            " <moneromooo> so you'd use only the outputs from n transactions and pay a fee for each output. that's eliminate all checks, and in case you stumble across something, you just need to remember it.\n",
            " <moneromooo> it would still require the output index, since you know where each output came from, for statistical purposes.\n",
            " <moneromooo> oh, not for that. because then you can rekey any output, if you don't remember the key image.\n",
            " <moneromooo> this would still be required for decoy output selection, but the two choices there would be:  ban all pids at carbon fork  or  allow --filter pids \n",
            " <sarang> if the fee and other parameters are set to simplify it for wallets, it seems like a no-brainer to move to that option\n",
            " <moneromooo> yes, if it is deemed appropriate.\n",
            " <sarang> if it's deemed unsafe or not useful, the latter is better\n",
            " <moneromooo> yes, let's do the study.\n",
            " <sarang> then we can move on to simplifying things a bit, and get back to the payment id stuff\n",
            " <sarang> i know suraenoether has been working on some efficient way to do encrypted timestamp=0 payments using clsag\n",
            " <luigi1111w> if we get better results on x then id wait for the fork\n",
            " <sarang> if we wait, no matter\n",
            " <sarang> fwiw the _same_ scheme will create a single output in mlsag_\n",
            " <sarang> \n",
            " <sarang> note that _only_ the key images of outputs will be used for payment ids, and not random data when generating subaddresses or other signature-related data\n",
            " <sarang> this was a design choice made for u.s. because it believed that providing the full text image  was the most practical level of obfuscation\n",
            " <sarang> for use with sekret\n",
            " <sarang> cool, has anyone else seen fit?\n",
            " <sarang> also: is the code public anywhere?\n",
            " <isthmus> can somebody extract the payment id from a monero transaction?\n",
            " <isthmus> i thought i could brute force the seed nodes among n nodes and retrieve the id “plus some other seeding data”?\n",
            " <moneromooo> add --fixed-fee n\n",
            " <isthmus> oh wait, it's --fixed-fee k in the gerber context, why?\n",
            " — isthmus digs around\n",
            " <isthmus> oh, that's right, it would also be possible to use the subaddresses thing right now but with a wallet specific key image\n",
            " <isthmus> not \"properly filterable\"\n",
            " <isthmus> or should that be --fixed-fee k\n",
            " <isthmus> not fully filtered\n",
            " <isthmus> i'm not really sure what's needed to filter it\n",
            " <moneromooo> filter it _as much as you want_ if necessary.\n",
            " <hyc> knaccc: yeah, that's basically what they were doing iirc with 128 bit payment ids, which was for a business use only\n",
            " <hyc> as a consumer of pids, you really don't care if they're using a payment id, because it is  not supposed to look up the id in the context of the purchase\n",
            " <hyc> the info in the tag would really identify if someone had already checked their subaddresses and found they were in use\n",
            " <hyc> not so much that it really identifies\n",
            " <hyc> but it does mean that if someone else has already had checked their\n",
            " <hyc> and found they are in use, you can't deny them\n",
            " <knaccc> hyc i don't agree with your first point, i think it should be optional. if you want to require a tag, you really need to have it. if you want to have a tag now, you can't really tell if it has one or not\n",
            " <knaccc> hyc yeah i don't want to move to a position where optional is so strongly encouraged, but\n",
            "\n",
            "[10760 | 52.84] loss=0.12 avg=0.12\n",
            "[10770 | 62.90] loss=0.43 avg=0.28\n",
            "[10780 | 72.98] loss=0.44 avg=0.33\n",
            "[10790 | 83.05] loss=0.25 avg=0.31\n",
            "[10800 | 93.13] loss=0.22 avg=0.29\n",
            "[10810 | 103.21] loss=0.19 avg=0.28\n",
            "[10820 | 113.28] loss=0.12 avg=0.25\n",
            "[10830 | 123.35] loss=0.59 avg=0.30\n",
            "[10840 | 133.43] loss=0.34 avg=0.30\n",
            "[10850 | 143.51] loss=0.72 avg=0.34\n",
            "[10860 | 153.59] loss=0.20 avg=0.33\n",
            "[10870 | 163.68] loss=0.24 avg=0.32\n",
            "[10880 | 173.77] loss=0.45 avg=0.33\n",
            "[10890 | 183.85] loss=0.17 avg=0.32\n",
            "[10900 | 193.95] loss=0.15 avg=0.31\n",
            "[10910 | 204.03] loss=0.24 avg=0.30\n",
            "[10920 | 214.11] loss=0.79 avg=0.33\n",
            "[10930 | 224.19] loss=0.68 avg=0.36\n",
            "[10940 | 234.27] loss=1.36 avg=0.41\n",
            "[10950 | 244.35] loss=0.71 avg=0.43\n",
            "[10960 | 254.42] loss=0.19 avg=0.42\n",
            "[10970 | 264.49] loss=0.47 avg=0.42\n",
            "[10980 | 274.57] loss=0.73 avg=0.44\n",
            "[10990 | 284.65] loss=0.17 avg=0.42\n",
            "[11000 | 294.73] loss=0.22 avg=0.41\n",
            "Saving checkpoint/run1774M/model-11000\n",
            "======== SAMPLE 1 ========\n",
            " outputs will need to double\n",
            " <sarang> suraenoether: ready to get started?\n",
            " <suraenoether> sure thing, buddy\n",
            " <suraenoether> welcome everyone, to the last mrl research meeting of the year\n",
            " <suraenoether> if i had thought about it, i'd have something more in-depth prepared but it just occurred to me ;p\n",
            " <suraenoether> let's start with 1) greetings\n",
            " <endogenic> o/\n",
            " <suraenoether> good and you, oh not so bad\n",
            " <serhack> hi!\n",
            " <koracain> hi\n",
            " <articmine> hi\n",
            " <atoc> hi\n",
            " <koracain> good\n",
            " <suraenoether> good and you, oh not so bad\n",
            " <suraenoether> isthmus: you requested a complete list of mrl publications for the year. i'm updating the ccs page\n",
            " <suraenoether> page 2) documents\n",
            " <suraenoether> yes\n",
            " <suraenoether> thecharlatan requested that i give an update on their status as of the last month\n",
            " <suraenoether> and i'll provide a brief update on the multisig paper, from their recent batching program\n",
            " <suraenoether> their goal is complete, however, before next month. many thanks!\n",
            " <suraenoether> koracain: let's chat about the coinbase paper in a bit\n",
            " <koracain> zkoracaincoin\n",
            " <lurkinandlearnin> what would be an ideal reading order for a paper/slides/etc\n",
            " <suraenoether> its a paper on my team to write up a summary of results after i get all the stuff aggregated and consolidated and do the multi-month thing. it's the kind of thing where we at least have consensus that this stuff is good, and it could go a long ways toward helping our community keep up to date with our research. it's not a unified \"best practices\" list, so we at least want to quantify our lack of privacy in multi-month, versus consensus # different platforms upon which we do rando\n",
            " <suraenoether> sarang, for example, one thing that is easy to talk about in a meeting is the lack of current consensus and anonymity size/fees. is it not nice that a txn fee of $0.002 does exist but doesn't make sense to one payer ? it still feels like a step backward, because one simple analysis tells you how many coinbase outputs on the chain last 30 days.\n",
            " <suraenoether> or the equivalent number of such transactions in the previous 6 months\n",
            " <suraenoether> i could enumerate  and tell you how many outputs i see there were within 2 hours\n",
            " <endogenic> suraenoether: monero is pretty well figured out with the bulletproofs thing now, afaik\n",
            " <sarang> i still have one unsigned bit about the security and efficiency question\n",
            " <sarang> but that can play nicely with the insight data i have\n",
            " <suraenoether> oh, i'm actually thinking fluently while retaining my poisson combonagement\n",
            " <suraenoether> anywya i have a feverish massagerous quantity of this morning\n",
            " <suraenoether> so i'll be working on mrl11 simulations and open-ness calculations\n",
            " <suraenoether> i need to capture both block size constraints and runtime constraints\n",
            " <suraenoether> suraenoether: let y be an oracle that responds to  in some way y is specified by the protocol\n",
            " <suraenoether> we can quantify how extensive that open-ness is or how well y breaks the computational hardness assumption\n",
            " <suraenoether> and we can quantify how that opens-ness is lost or gained\n",
            " <suraenoether> so i'm writing up a quick script to generate a chaos matrix of block size r, and i'm also plotting the contribls of a half-eat for a half-dose of y.\n",
            " <suraenoether> the first one to plot a figure of this distribution will give us a good idea of what the error at this point is\n",
            " <suraenoether> \n",
            " <suraenoether> the second one to plot will give us beta at this density\n",
            " <suraenoether> this will give us beta: the 2nd half to an estimate of how well this sampling 'gamma' distribution is working\n",
            " <suraenoether> i'm estimating this value\n",
            " <suraenoether> one reason i like using these statistics is that they are not nearly as sensitive as we might like to one of these values, depending on how well the sampling is\n",
            " <suraenoether> so i want\n",
            "\n",
            "[11010 | 356.52] loss=0.21 avg=0.40\n",
            "[11020 | 366.60] loss=0.21 avg=0.40\n",
            "[11030 | 376.68] loss=0.22 avg=0.39\n",
            "[11040 | 386.77] loss=0.28 avg=0.38\n",
            "[11050 | 396.85] loss=0.29 avg=0.38\n",
            "[11060 | 406.92] loss=0.40 avg=0.38\n",
            "[11070 | 417.00] loss=0.58 avg=0.39\n",
            "[11080 | 427.08] loss=0.24 avg=0.38\n",
            "[11090 | 437.17] loss=1.37 avg=0.42\n",
            "[11100 | 447.24] loss=0.72 avg=0.43\n",
            "[11110 | 457.33] loss=0.18 avg=0.42\n",
            "[11120 | 467.41] loss=0.93 avg=0.44\n",
            "[11130 | 477.49] loss=0.80 avg=0.45\n",
            "[11140 | 487.57] loss=0.39 avg=0.45\n",
            "[11150 | 497.66] loss=0.16 avg=0.44\n",
            "[11160 | 507.76] loss=0.60 avg=0.44\n",
            "[11170 | 517.84] loss=0.25 avg=0.44\n",
            "[11180 | 527.92] loss=0.24 avg=0.43\n",
            "[11190 | 538.00] loss=0.32 avg=0.43\n",
            "[11200 | 548.10] loss=0.39 avg=0.43\n",
            "[11210 | 558.18] loss=0.33 avg=0.42\n",
            "[11220 | 568.26] loss=0.39 avg=0.42\n",
            "[11230 | 578.34] loss=0.59 avg=0.43\n",
            "[11240 | 588.42] loss=0.41 avg=0.43\n",
            "[11250 | 598.50] loss=0.43 avg=0.43\n",
            "======== SAMPLE 1 ========\n",
            " < ( <suraenoether> it's a good thought\n",
            " <suraenoether> i would've called it homomorphic when i first thought of the idea, but the algorithm i came up with seems to have a few structural problems\n",
            " <kenshamir> right, what was the problem?\n",
            " <kenshamir> i seem to recall that you had some mathematical questions or a project you were working on that made him dip into your work, which did not make sense\n",
            " <sarang> ?\n",
            " <kenshamir> <sarang \"?\"> what time was the conversation?\n",
            " <mochi101> thanks\n",
            " <sarang> good morning, all\n",
            " <kenshamir> daw as it goes?\n",
            " <sarang> yeah! i'll be working on paper today and checking my scripts\n",
            " <mochi101> ok, what time?\n",
            " <kenshamir> ok too. i can do a quick rundown of what you did in the pipeline, in case you want to update us\n",
            " <suraenoether> being able to publish results like this without the support of your university will be nice, and i'm very excited to have x for that for many years to come\n",
            " <suraenoether> being able to support that kind of thing in a very solid way will be my big goal this year\n",
            " <sarang> what is their support?\n",
            " <sarang> *raises eyebrow*\n",
            " <suraenoether> we have a lot of interesting stuff, it's just... you know, in general, if it's not a case of \"you can read it or use it or whatever\", then it's a totally different thing. *cough* that came from a dream\n",
            " <kenshamir> > ** your message\n",
            " <suraenoether> kennonero asked: we have a lot of interesting stuff, it's just... you know, in general, if it's not a case of \"you can read it or use it or whatever\", then it's a totally different thing. *cough* there are stacks of papers that do that sort of thing, it's how they are published, in any case\n",
            " <suraenoether> yeah, i'm just going to put it down and hope for the best :d\n",
            " <kenshamir> <sarang \"what is their support?\"> if they can't support monero or zcash, then i don't know how to explain why you can support monero or zcash. can you help?\n",
            " <suraenoether> i'm not asking them directly\n",
            " <hyc> \"i'm just going to put it down and hope for the best :d\"\n",
            " <kenshamir> i'm just wondering what their state of support boils down to in the next few years, because we don't seem to be doing the same support for them as for other projects\n",
            " <hyc> they know that they have a team that recently ran a $3b business, but how do they profit from it?\n",
            " <kenshamir> and the fact that you don't really care what the company does...\n",
            " <hyc> if you care about something that makes monero useable, then you can care about whatever the other monero forks or other coins are doing to the project.\n",
            " <sarang> i really don't like the idea of people using their work in some kind of funding round\n",
            " <sarang> it makes monero seem like a company, not like a tool\n",
            " <hyc> we're not even aCompany. i don't know that.\n",
            " <hyc> if you are willing to make sesne and make the cost of your own software do some things that aren't actually yours... you may be ok, but i dont think so.\n",
            " <sarang> fwiw i don't really support the idea\n",
            " <sarang> \"we just have to offer something that's inadequate\"\n",
            " <hyc> it's not a question. if something is not yours, it's not really a security problem\n",
            " <sarang> i don't know the specific issue they're looking towards, but the idea seems like it's a case of \"the core team runs a thing that they want to sell to us\"\n",
            " <sarang> not really a problem in their minds, of course\n",
            " <suraenoether> kennonero: we have no business contract. hehe\n",
            " <kenshamir> not sure what his question is, do you have any ideas?\n",
            " <kenshamir> i guess it's failure itself to understand it\n",
            " <sarang> i can't think of any\n",
            " <kenshamir> yep i think it's \"how much can they charge us and what can they charge us for our work\"\n",
            " <kenshamir> <sarang \"fwiw i don't really\n",
            "\n",
            "[11260 | 633.44] loss=0.50 avg=0.43\n",
            "[11270 | 643.51] loss=0.29 avg=0.43\n",
            "[11280 | 653.59] loss=0.40 avg=0.43\n",
            "[11290 | 663.67] loss=0.29 avg=0.42\n",
            "[11300 | 673.74] loss=0.30 avg=0.42\n",
            "[11310 | 683.83] loss=0.37 avg=0.42\n",
            "[11320 | 693.90] loss=0.55 avg=0.42\n",
            "[11330 | 703.99] loss=0.10 avg=0.41\n",
            "interrupted\n",
            "Saving checkpoint/run1774M/model-11337\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHdTL8NDbAh3"
      },
      "source": [
        "## To save a model to gdrive:\n",
        "\n",
        "# gpt2.copy_checkpoint_to_gdrive(run_name=this_run_name)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD"
      },
      "source": [
        "# # To load a model from gdrive:\n",
        "\n",
        "# gpt2.copy_checkpoint_from_gdrive(run_name=this_run_name)\n",
        "# sess = gpt2.start_tf_sess()\n",
        "# gpt2.load_gpt2(sess, run_name=this_run_name)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp"
      },
      "source": [
        "## Generate Text From The Trained Model\n",
        "\n",
        "Unseeded example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RNY6RBI9LmL",
        "outputId": "2ff2b5cd-a4c4-46a1-87c9-07c94c46d7d2"
      },
      "source": [
        "gpt2.generate(sess, run_name=this_run_name)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " His\n",
            " <moneromooo> i think the reason is that it was added very late, when bytecoin realised some \"client distinguishability\" was needed.\n",
            " <moneromooo> i think the intent was to offer a layer of \"this transaction was built with good intelligence, and i don't know why\" that would be helpful.\n",
            " <sarang> the r value would be for the sender, not the receiver\n",
            " <sarang> and it's important that the sender know the r value\n",
            " <sarang> the receiver could be influenced by the use of an index that is linked to the true signing index, which can't be easily verified anyway\n",
            " <sarang> the point is that the way the indexing appears in the signature is *good* but doesn't matter for the receiver\n",
            " <sarang> the signer should include a hash of all the indices used, so the receiver knows, say, the index for the true signer\n",
            " <sarang> i'm thinking through the consequences to the current use of the non-indexed indices in the ring\n",
            " <sarang> \n",
            " <sarang> and there's no good reason other than index linking\n",
            " <sarang> this needs careful thought\n",
            " <sarang> for triptych it means the need for the offset\n",
            " <sarang> otherwise the signer could \"waste\" the signer's time by using the wrong hash\n",
            " <sarang> at least, i think, the signer should include a hash of the indices used, so the signer knows how to use the right hash\n",
            " <sarang> i don't know what other options are good, but here's a small one:\n",
            " <sarang> i'm investigating how to strip pids off the matrix code and the test implementation does not allow them\n",
            " <sarang> any other observations on the search for info?\n",
            " <nioc> i still need to run the data on pool outputs. not sure if the answer will be found soon\n",
            " <sarang> good to have if you can provide it\n",
            " <sarang> ok, then i'm not sure what else to do\n",
            " <sarang> so a node running ccs will always have the required data \n",
            " <sarang> \n",
            " <sarang> but then, if it gets built, the node could do a single algo for the pids\n",
            " <sarang> since the signature is unique per signature, it only has to include them in that transaction\n",
            " <sarang> can you expand on this?\n",
            " <nioc> hmm. i'm not sure i understand\n",
            " <nioc> i will need more reading\n",
            " <nioc> i am assuming that there are no other ways to reach the goal let's go ahead and sign it off\n",
            " <sarang> sure, just providing the node software the data it needs to verify signatures and tend to only sign things it believes are true\n",
            " <sarang> it won't know what other signatures are or aren't\n",
            " <sarang> it will just reject new signatures that it doesn't recognize\n",
            " <sarang> doesn't really matter\n",
            " <sarang> so the definition only really applies to new sigs\n",
            " <nioc> right but a node can be a schnorr fast node\n",
            " <sarang> no, but a schnorr fast node is just a schnorr signature, and not a node at all\n",
            " <nioc> existed\n",
            " <nioc> i thought long ago about using schnorr's sig for the index as well but i don't think it would work\n",
            " <nioc> so the other discussion is to use a schnorr signature for the point at infinity for the one below index\n",
            " <sarang> using the signature is only ever used to ensure the signing index is actually the signer, of course\n",
            " <sarang> it's used far less often than we expect\n",
            " <nioc> hi, just asked out of curiosity, was this the work of a previous member of the ccs?\n",
            " <sarang> yes\n",
            " <nioc> thx\n",
            " <sarang> sorry, does the code not work for triptych?\n",
            " <sarang> i had to add in pre-image checks, and didn't want to introduce the new checked-in version\n",
            " <sarang> \n",
            " <sarang> https://github.com/sarangnoether/monero/blob/triptych-aggregate-pippenger/src/ringct/triptych.rs#l723\n",
            " <sarang> adding the pre-image checks is only done for the new aggregated version, because it's more efficient\n",
            " <sarang> but it's harmless\n",
            " <sarang> s/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R"
      },
      "source": [
        "Parameters for `gpt2.generate`:\n",
        "\n",
        "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
        "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
        "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N"
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              length=400,\n",
        "              temperature=0.7,\n",
        "              # prefix=\" <handle>\",\n",
        "              # top_p = 0.9,\n",
        "              nsamples=10,\n",
        "              batch_size=5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zSfqZ-BxlIz"
      },
      "source": [
        "## Ask GPT-MRL about specific topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AzEYuJLxkSD"
      },
      "source": [
        "topics = ['meeting',\n",
        "          'scalarmult',\n",
        "          'quantum',\n",
        "          'bulletproof',\n",
        "          'dynamic block',\n",
        "          'triptych',\n",
        "          'clsag',\n",
        "          'mlsag',\n",
        "          'arcturus',\n",
        "          'ring signature',\n",
        "          'zero knowledge',\n",
        "          'anonymity set',\n",
        "          ''\n",
        "          ]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ornbh394zm5U"
      },
      "source": [
        "for t in topics:\n",
        "  print((\"*\"*25 + \"\\n\")*2 + \"MRL-AI (GPT-2) on \" + t + \":\\n\")\n",
        "  gpt2.generate(sess,\n",
        "              length=750,\n",
        "              temperature=0.7,\n",
        "              prefix=t,\n",
        "              include_prefix=False,\n",
        "              run_name = this_run_name,\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              #top_p = 0.9\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjEN2Tafhl2"
      },
      "source": [
        "For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n",
        "\n",
        "You can rerun the cells as many times as you want for even more generated texts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6p6arifSL0"
      },
      "source": [
        "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-LRex8lfv1g"
      },
      "source": [
        "# may have to run twice to get file to download\n",
        "files.download(gen_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmTXWNUygS5E"
      },
      "source": [
        "# LICENSE\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2019 Max Woolf\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    }
  ]
}