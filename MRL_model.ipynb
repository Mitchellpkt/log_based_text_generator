{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monero Research Lab text generator\n",
    "Isthmus / Mithchell\n",
    "\n",
    "Modification of text generator code from Pranjal Srivastava, see https://www.analyticsvidhya.com/blog/2018/03/text-generation-using-python-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning and processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RNN\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# IRC Log processing\n",
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_path = \"mrl_logs_raw.txt\"\n",
    "most_recent_N_characters = 10**5\n",
    "savelogs = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "manip_text_raw = (open(log_file_path).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process IRC logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode\n",
      "timestamp\n",
      "joined\n",
      "left\n",
      "quit\n",
      "seconds\n",
      "channel\n",
      "#monero-research-lab\n",
      "→\n"
     ]
    }
   ],
   "source": [
    "# Drop case\n",
    "manip_text_raw = manip_text_raw.lower() # drop case\n",
    "\n",
    "# REmove channel notifications\n",
    "words_to_remove = ('mode','timestamp','joined','left','quit','seconds','channel', '#monero-research-lab', '→')\n",
    "manip_text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", manip_text_raw) # remove timestamps\n",
    "for w in range(len(words_to_remove)):\n",
    "    this_word = words_to_remove[w]\n",
    "    print(this_word)\n",
    "    manip_text = re.sub(\".*\"+this_word+\".*\", \"\", manip_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " <ukoehb> is transaction fee 8 bytes?\n",
      " <moneromooo> it is a 64 bit value. it is typically encoded as a varint, if that's what you're asking.\n",
      " <ukoehb> just looking at storage required\n",
      " <ukoehb> varint = variable length integer, so is storage not constant?\n",
      " <moneromooo> yes.\n",
      " <ukoehb> thanks :)\n",
      " <serhack> morning :)\n",
      " <suraenoether> monero coffee chat yall~\n",
      " <sarang> how did the coffee chat go?\n",
      " <sarang> i had a volunteer commitment during that time\n",
      " <sarang> we repair bikes and donate them to veterans and kids who need them\n",
      " <sneurlax1> good with bikes, eh?\n",
      " <sarang> i worked part-time as a mechanic for a few years\n",
      " <sneurlax1> i missed the meeting so have no useful comment there sorry.\n",
      " <sarang> fixing bikes is a ton of fun\n",
      " <sneurlax1> i skipped straight to motorcycles and need to get handy with it quickly\n",
      " — sarang is moving bike convo to #monero-research-lounge \n",
      " <needmoney90> my call with bisq is wednesday, would anyone be available to chat about the technical details of how multi\n"
     ]
    }
   ],
   "source": [
    "max_rows_blank = 200\n",
    "for i in range(max_rows_blank):\n",
    "    search_str = \"\\n\"*(max_rows_blank-i)\n",
    "    manip_text = re.sub(search_str,'\\n',manip_text)\n",
    "    \n",
    "# Peep the results\n",
    "print(manip_text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1==1:\n",
    "    text_file = open(\"mrl_logs_processed.txt\", \"w\")\n",
    "    text_file.write(manip_text)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep the most recent logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7001579\n"
     ]
    }
   ],
   "source": [
    "mlen = len(manip_text)\n",
    "print(mlen)\n",
    "text = manip_text[(mlen-most_recent_N_characters):(most_recent_N_characters)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side quest: handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_carrots = re.findall(\"\\<.*\\>\", manip_text_raw)\n",
    "handles_weighted = [x for x in all_carrots if (\" \" in x) == False]\n",
    "unique_handles = list(set(handles_weighted))\n",
    "\n",
    "# Peep the results\n",
    "unique_handles\n",
    "\n",
    "# Save if desired\n",
    "if savelogs:\n",
    "    text_file = open(\"mrl_logs_handles.txt\", \"w\")\n",
    "    text_file.write(str(unique_handles[:]))\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create character/word mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = sorted(list(set(text)))\n",
    "\n",
    "n_to_char = {n:char for n, char in enumerate(characters)}\n",
    "char_to_n = {char:n for n, char in enumerate(characters)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "length = len(text)\n",
    "seq_length = 100\n",
    "\n",
    "for i in range(0, length-seq_length, 1):\n",
    "    sequence = text[i:i + seq_length]\n",
    "    label =text[i + seq_length]\n",
    "    X.append([char_to_n[char] for char in sequence])\n",
    "    Y.append(char_to_n[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_modified = np.reshape(X, (len(X), seq_length, 1))\n",
    "X_modified = X_modified / float(len(characters))\n",
    "Y_modified = np_utils.to_categorical(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_wide = 300 # 400 default, 700 wider\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(num_wide, input_shape=(X_modified.shape[1], X_modified.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(num_wide, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(num_wide))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(Y_modified.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1998/1998 [==============================] - 1648s 823ms/step - loss: 3.2078\n",
      "Epoch 2/25\n",
      "1998/1998 [==============================] - 1629s 815ms/step - loss: 2.8594\n",
      "Epoch 3/25\n",
      "1998/1998 [==============================] - 1625s 813ms/step - loss: 2.6207\n",
      "Epoch 4/25\n",
      "1998/1998 [==============================] - 1632s 817ms/step - loss: 2.4328\n",
      "Epoch 5/25\n",
      "1998/1998 [==============================] - 1641s 821ms/step - loss: 2.2800\n",
      "Epoch 6/25\n",
      "1998/1998 [==============================] - 1642s 822ms/step - loss: 2.1624\n",
      "Epoch 7/25\n",
      "1998/1998 [==============================] - 1639s 820ms/step - loss: 2.0731\n",
      "Epoch 8/25\n",
      "1998/1998 [==============================] - 1644s 823ms/step - loss: 1.9916\n",
      "Epoch 9/25\n",
      "1998/1998 [==============================] - 1647s 824ms/step - loss: 1.9309\n",
      "Epoch 10/25\n",
      "1998/1998 [==============================] - 1649s 825ms/step - loss: 1.8646\n",
      "Epoch 11/25\n",
      "1998/1998 [==============================] - 1639s 820ms/step - loss: 1.8180\n",
      "Epoch 12/25\n",
      "1998/1998 [==============================] - 1646s 824ms/step - loss: 1.7760\n",
      "Epoch 13/25\n",
      "1998/1998 [==============================] - 1645s 823ms/step - loss: 1.7419\n",
      "Epoch 14/25\n",
      "1998/1998 [==============================] - 1636s 819ms/step - loss: 1.6914\n",
      "Epoch 15/25\n",
      "1998/1998 [==============================] - 1649s 825ms/step - loss: 1.6649\n",
      "Epoch 16/25\n",
      "1998/1998 [==============================] - 1657s 829ms/step - loss: 1.6306\n",
      "Epoch 17/25\n",
      "1998/1998 [==============================] - 1642s 822ms/step - loss: 1.5964\n",
      "Epoch 18/25\n",
      "1998/1998 [==============================] - 1642s 822ms/step - loss: 1.5809\n",
      "Epoch 19/25\n",
      "1998/1998 [==============================] - 1664s 833ms/step - loss: 1.5563\n",
      "Epoch 20/25\n",
      "1998/1998 [==============================] - 1656s 829ms/step - loss: 1.5178\n",
      "Epoch 21/25\n",
      "1998/1998 [==============================] - 1644s 823ms/step - loss: 1.4890\n",
      "Epoch 22/25\n",
      "1998/1998 [==============================] - 1662s 832ms/step - loss: 1.4666\n",
      "Epoch 23/25\n",
      "1998/1998 [==============================] - 1648s 825ms/step - loss: 1.4537\n",
      "Epoch 24/25\n",
      "1998/1998 [==============================] - 1667s 834ms/step - loss: 1.4320\n",
      "Epoch 25/25\n",
      "1998/1998 [==============================] - 1632s 817ms/step - loss: 1.3998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f369058ccd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 25# epochs 100 default\n",
    "batch_sizes = 50 # batch size 50 default\n",
    "\n",
    "model.fit(X_modified, Y_modified, epochs=num_epochs, batch_size=batch_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_mapped = X[99] # 99\n",
    "full_string = [n_to_char[value] for value in string_mapped]\n",
    "# generating characters\n",
    "for i in range(400):\n",
    "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
    "    x = x / float(len(characters))\n",
    "\n",
    "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
    "    seq = [n_to_char[value] for value in string_mapped]\n",
    "    full_string.append(n_to_char[pred_index])\n",
    "\n",
    "    string_mapped.append(pred_index)\n",
    "    string_mapped = string_mapped[1:len(string_mapped)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ractical details by ecc researchers\\n <sarang> inge-: yeah, nothing has changed\\n <inge-> and halo 2 in the seal output is also a blockchain is a blockchain is a blockchain in the seal output is also a blockchain is a blockchain is a blockchain in the seal output is also a blockchain is a blockchain is a blockchain in the seal output is also a blockchain is a blockchain is a blockchain in the seal output is also a blockchain is a blockchain is a blockchain in the seal output is also a blockchain is a '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combining text\n",
    "txt=\"\"\n",
    "for char in full_string:\n",
    "    txt = txt+char\n",
    "txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop to extract more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_var = 100\n",
    "xarray = [0,99,100,200,500,1000,2000,5000,10000,20000,50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "point.\n",
      " <sarang> some community experts asked for details on their forums, but were not given any proposal donstolled be a blockchain is a blockchain is a blockchain is a blockchain is a blockchain is a bl\n",
      "99\n",
      "ractical details by ecc researchers\n",
      " <sarang> inge-: yeah, nothing has changed\n",
      " <inge-> and halo 2 in the seal output is a blockchain is a blockchain is a blockchain in the seal output is also a blockchain \n",
      "100\n",
      "actical details by ecc researchers\n",
      " <sarang> inge-: yeah, nothing has changed\n",
      " <inge-> and halo 2 is the seal output is also a blockchain is a blockchain is a blockchain in the seal output is also a block\n",
      "200\n",
      " the only horse in the race currently?\n",
      " <sarang> you can see their repo, but all the commits and prsput is also alockchain is a blockchain is a blockchain  <sarang> i don't see the seal output is a bloc\n",
      "500\n",
      "ct\n",
      " <inge-> no i mean in general, things that could be a future candidate for monero\n",
      " <sarang> oh oke of the seal output is a blockchain is a blockchain in the seal output is also a blockchain is a bloc\n",
      "1000\n",
      "t doesn't automatically make everything fast and small forever\n",
      " <sarang> you need a construction that i see a probers of the seal output is also a blockchain is a blockchain is a blockchain is a blockch\n",
      "2000\n",
      "is like ketchup. the main course is usually the information theoretical part and the sides being the seal output is also a blockchain is a blockchain is a blockchain in the seal output is also a block\n",
      "5000\n",
      "st didn't work\n",
      " <hyc> sgp: you decided 0/0 is 100% ?\n",
      " <sarang> hyc: fwiw i don't agree that ecc research output is a blockchain is a blockchain is a blockchain is a blockchain in the seal output is al\n",
      "10000\n",
      " complexity in the market process, which would seem to make it inherently more efficient\n",
      " <sarang> weah, i think it is a blockchain is a blockchain is a blockchain is a blockchain is a blockchain is a\n",
      "20000\n",
      " who spent $10,000 and 2-3 years of their life on their courses\n",
      " <ktkxxeewr> and you get a bunch of the seal output is a blockchain is a blockchain is a blockchain is a blockchain is a blockchain is a\n",
      "50000\n",
      "i hope nobody is falling for that\n",
      " <netrik182> not visible on matrix erciccione\n",
      " <erciccione> should be a block thme is a blockchain is a blockchain is a blockchain is a blockchain is a blockchain is \n"
     ]
    }
   ],
   "source": [
    "for x in xarray: # range(5):\n",
    "    print(x)\n",
    "    \n",
    "    string_mapped = X[x] # 99\n",
    "    full_string = [n_to_char[value] for value in string_mapped]\n",
    "    \n",
    "    # generating characters\n",
    "    for i in range(length_var):\n",
    "        x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
    "        x = x / float(len(characters))\n",
    "\n",
    "        pred_index = np.argmax(model.predict(x, verbose=0))\n",
    "        seq = [n_to_char[value] for value in string_mapped]\n",
    "        full_string.append(n_to_char[pred_index])\n",
    "\n",
    "        string_mapped.append(pred_index)\n",
    "        string_mapped = string_mapped[1:len(string_mapped)]\n",
    "        \n",
    "    #combining text\n",
    "    txt=\"\"\n",
    "    for char in full_string:\n",
    "        txt = txt+char\n",
    "        \n",
    "    print(txt)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
